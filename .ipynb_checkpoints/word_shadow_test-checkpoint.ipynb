{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f50bf05-feed-4404-90fe-56e88e5ee505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 175.0394\n",
      "Epoch 6, Loss: 174.9886\n",
      "Epoch 11, Loss: 174.9336\n",
      "Epoch 16, Loss: 174.8722\n",
      "Epoch 21, Loss: 174.8014\n",
      "Epoch 26, Loss: 174.7178\n",
      "Epoch 31, Loss: 174.6172\n",
      "Epoch 36, Loss: 174.4941\n",
      "Epoch 41, Loss: 174.3413\n",
      "Epoch 46, Loss: 174.1500\n",
      "Epoch 51, Loss: 173.9086\n",
      "Epoch 56, Loss: 173.6025\n",
      "Epoch 61, Loss: 173.2136\n",
      "Epoch 66, Loss: 172.7202\n",
      "Epoch 71, Loss: 172.0974\n",
      "Epoch 76, Loss: 171.3189\n",
      "Epoch 81, Loss: 170.3613\n",
      "Epoch 86, Loss: 169.2105\n",
      "Epoch 91, Loss: 167.8715\n",
      "Epoch 96, Loss: 166.3770\n",
      "Epoch 101, Loss: 164.7874\n",
      "Epoch 106, Loss: 163.1752\n",
      "Epoch 111, Loss: 161.5965\n",
      "Epoch 116, Loss: 160.0702\n",
      "Epoch 121, Loss: 158.5782\n",
      "Epoch 126, Loss: 157.0809\n",
      "Epoch 131, Loss: 155.5332\n",
      "Epoch 136, Loss: 153.8939\n",
      "Epoch 141, Loss: 152.1298\n",
      "Epoch 146, Loss: 150.2192\n",
      "Epoch 151, Loss: 148.1556\n",
      "Epoch 156, Loss: 145.9547\n",
      "Epoch 161, Loss: 143.6611\n",
      "Epoch 166, Loss: 141.3506\n",
      "Epoch 171, Loss: 139.1180\n",
      "Epoch 176, Loss: 137.0506\n",
      "Epoch 181, Loss: 135.2026\n",
      "Epoch 186, Loss: 133.5890\n",
      "Epoch 191, Loss: 132.1981\n",
      "Epoch 196, Loss: 131.0058\n",
      "Epoch 201, Loss: 129.9844\n",
      "Epoch 206, Loss: 129.1073\n",
      "Epoch 211, Loss: 128.3510\n",
      "Epoch 216, Loss: 127.6950\n",
      "Epoch 221, Loss: 127.1219\n",
      "Epoch 226, Loss: 126.6175\n",
      "Epoch 231, Loss: 126.1700\n",
      "Epoch 236, Loss: 125.7697\n",
      "Epoch 241, Loss: 125.4088\n",
      "Epoch 246, Loss: 125.0809\n",
      "Epoch 251, Loss: 124.7809\n",
      "Epoch 256, Loss: 124.5048\n",
      "Epoch 261, Loss: 124.2492\n",
      "Epoch 266, Loss: 124.0114\n",
      "Epoch 271, Loss: 123.7891\n",
      "Epoch 276, Loss: 123.5807\n",
      "Epoch 281, Loss: 123.3845\n",
      "Epoch 286, Loss: 123.1992\n",
      "Epoch 291, Loss: 123.0238\n",
      "Epoch 296, Loss: 122.8574\n",
      "Epoch 301, Loss: 122.6991\n",
      "Epoch 306, Loss: 122.5482\n",
      "Epoch 311, Loss: 122.4042\n",
      "Epoch 316, Loss: 122.2665\n",
      "Epoch 321, Loss: 122.1346\n",
      "Epoch 326, Loss: 122.0081\n",
      "Epoch 331, Loss: 121.8866\n",
      "Epoch 336, Loss: 121.7698\n",
      "Epoch 341, Loss: 121.6573\n",
      "Epoch 346, Loss: 121.5490\n",
      "Epoch 351, Loss: 121.4444\n",
      "Epoch 356, Loss: 121.3435\n",
      "Epoch 361, Loss: 121.2459\n",
      "Epoch 366, Loss: 121.1516\n",
      "Epoch 371, Loss: 121.0602\n",
      "Epoch 376, Loss: 120.9717\n",
      "Epoch 381, Loss: 120.8859\n",
      "Epoch 386, Loss: 120.8026\n",
      "Epoch 391, Loss: 120.7217\n",
      "Epoch 396, Loss: 120.6431\n",
      "Epoch 401, Loss: 120.5667\n",
      "Epoch 406, Loss: 120.4924\n",
      "Epoch 411, Loss: 120.4200\n",
      "Epoch 416, Loss: 120.3495\n",
      "Epoch 421, Loss: 120.2808\n",
      "Epoch 426, Loss: 120.2138\n",
      "Epoch 431, Loss: 120.1485\n",
      "Epoch 436, Loss: 120.0847\n",
      "Epoch 441, Loss: 120.0224\n",
      "Epoch 446, Loss: 119.9615\n",
      "Epoch 451, Loss: 119.9020\n",
      "Epoch 456, Loss: 119.8438\n",
      "Epoch 461, Loss: 119.7868\n",
      "Epoch 466, Loss: 119.7311\n",
      "Epoch 471, Loss: 119.6765\n",
      "Epoch 476, Loss: 119.6231\n",
      "Epoch 481, Loss: 119.5707\n",
      "Epoch 486, Loss: 119.5193\n",
      "Epoch 491, Loss: 119.4690\n",
      "Epoch 496, Loss: 119.4196\n",
      "Epoch 501, Loss: 119.3711\n",
      "Epoch 506, Loss: 119.3235\n",
      "Epoch 511, Loss: 119.2768\n",
      "Epoch 516, Loss: 119.2310\n",
      "Epoch 521, Loss: 119.1859\n",
      "Epoch 526, Loss: 119.1417\n",
      "Epoch 531, Loss: 119.0982\n",
      "Epoch 536, Loss: 119.0554\n",
      "Epoch 541, Loss: 119.0134\n",
      "Epoch 546, Loss: 118.9720\n",
      "Epoch 551, Loss: 118.9314\n",
      "Epoch 556, Loss: 118.8914\n",
      "Epoch 561, Loss: 118.8520\n",
      "Epoch 566, Loss: 118.8133\n",
      "Epoch 571, Loss: 118.7751\n",
      "Epoch 576, Loss: 118.7376\n",
      "Epoch 581, Loss: 118.7007\n",
      "Epoch 586, Loss: 118.6643\n",
      "Epoch 591, Loss: 118.6286\n",
      "Epoch 596, Loss: 118.5933\n",
      "Epoch 601, Loss: 118.5586\n",
      "Epoch 606, Loss: 118.5245\n",
      "Epoch 611, Loss: 118.4908\n",
      "Epoch 616, Loss: 118.4577\n",
      "Epoch 621, Loss: 118.4251\n",
      "Epoch 626, Loss: 118.3931\n",
      "Epoch 631, Loss: 118.3615\n",
      "Epoch 636, Loss: 118.3304\n",
      "Epoch 641, Loss: 118.2998\n",
      "Epoch 646, Loss: 118.2697\n",
      "Epoch 651, Loss: 118.2401\n",
      "Epoch 656, Loss: 118.2110\n",
      "Epoch 661, Loss: 118.1824\n",
      "Epoch 666, Loss: 118.1542\n",
      "Epoch 671, Loss: 118.1266\n",
      "Epoch 676, Loss: 118.0994\n",
      "Epoch 681, Loss: 118.0727\n",
      "Epoch 686, Loss: 118.0465\n",
      "Epoch 691, Loss: 118.0207\n",
      "Epoch 696, Loss: 117.9954\n",
      "Epoch 701, Loss: 117.9706\n",
      "Epoch 706, Loss: 117.9463\n",
      "Epoch 711, Loss: 117.9224\n",
      "Epoch 716, Loss: 117.8990\n",
      "Epoch 721, Loss: 117.8761\n",
      "Epoch 726, Loss: 117.8536\n",
      "Epoch 731, Loss: 117.8315\n",
      "Epoch 736, Loss: 117.8100\n",
      "Epoch 741, Loss: 117.7888\n",
      "Epoch 746, Loss: 117.7681\n",
      "Epoch 751, Loss: 117.7479\n",
      "Epoch 756, Loss: 117.7281\n",
      "Epoch 761, Loss: 117.7087\n",
      "Epoch 766, Loss: 117.6897\n",
      "Epoch 771, Loss: 117.6712\n",
      "Epoch 776, Loss: 117.6530\n",
      "Epoch 781, Loss: 117.6352\n",
      "Epoch 786, Loss: 117.6179\n",
      "Epoch 791, Loss: 117.6009\n",
      "Epoch 796, Loss: 117.5843\n",
      "Epoch 801, Loss: 117.5681\n",
      "Epoch 806, Loss: 117.5522\n",
      "Epoch 811, Loss: 117.5366\n",
      "Epoch 816, Loss: 117.5215\n",
      "Epoch 821, Loss: 117.5066\n",
      "Epoch 826, Loss: 117.4921\n",
      "Epoch 831, Loss: 117.4779\n",
      "Epoch 836, Loss: 117.4640\n",
      "Epoch 841, Loss: 117.4504\n",
      "Epoch 846, Loss: 117.4371\n",
      "Epoch 851, Loss: 117.4241\n",
      "Epoch 856, Loss: 117.4114\n",
      "Epoch 861, Loss: 117.3989\n",
      "Epoch 866, Loss: 117.3867\n",
      "Epoch 871, Loss: 117.3747\n",
      "Epoch 876, Loss: 117.3631\n",
      "Epoch 881, Loss: 117.3516\n",
      "Epoch 886, Loss: 117.3404\n",
      "Epoch 891, Loss: 117.3294\n",
      "Epoch 896, Loss: 117.3187\n",
      "Epoch 901, Loss: 117.3081\n",
      "Epoch 906, Loss: 117.2978\n",
      "Epoch 911, Loss: 117.2877\n",
      "Epoch 916, Loss: 117.2778\n",
      "Epoch 921, Loss: 117.2681\n",
      "Epoch 926, Loss: 117.2586\n",
      "Epoch 931, Loss: 117.2493\n",
      "Epoch 936, Loss: 117.2402\n",
      "Epoch 941, Loss: 117.2312\n",
      "Epoch 946, Loss: 117.2224\n",
      "Epoch 951, Loss: 117.2138\n",
      "Epoch 956, Loss: 117.2054\n",
      "Epoch 961, Loss: 117.1972\n",
      "Epoch 966, Loss: 117.1891\n",
      "Epoch 971, Loss: 117.1812\n",
      "Epoch 976, Loss: 117.1734\n",
      "Epoch 981, Loss: 117.1658\n",
      "Epoch 986, Loss: 117.1583\n",
      "Epoch 991, Loss: 117.1510\n",
      "Epoch 996, Loss: 117.1439\n",
      "Epoch 1000, Loss: 117.1383\n",
      "\n",
      "ðŸ” Inferred 'nouniness' of words (based on preceding word prediction):\n",
      "the        â†’ noun-score: 0.998\n",
      "also       â†’ noun-score: 0.961\n",
      "with       â†’ noun-score: 0.934\n",
      "run        â†’ noun-score: 0.855\n",
      "play       â†’ noun-score: 0.816\n",
      "barked     â†’ noun-score: 0.773\n",
      "many       â†’ noun-score: 0.747\n",
      "in         â†’ noun-score: 0.695\n",
      "dogs       â†’ noun-score: 0.555\n",
      "some       â†’ noun-score: 0.517\n",
      "sat        â†’ noun-score: 0.471\n",
      "a          â†’ noun-score: 0.460\n",
      "jumped     â†’ noun-score: 0.459\n",
      "on         â†’ noun-score: 0.444\n",
      "trees      â†’ noun-score: 0.443\n",
      "high       â†’ noun-score: 0.434\n",
      "loudly     â†’ noun-score: 0.428\n",
      "fast       â†’ noun-score: 0.311\n",
      "birds      â†’ noun-score: 0.216\n",
      "are        â†’ noun-score: 0.214\n",
      "fly        â†’ noun-score: 0.212\n",
      "sing       â†’ noun-score: 0.211\n",
      "dog        â†’ noun-score: 0.195\n",
      "children   â†’ noun-score: 0.154\n",
      "mat        â†’ noun-score: 0.036\n",
      "cat        â†’ noun-score: 0.036\n",
      "girl       â†’ noun-score: 0.036\n",
      "garden     â†’ noun-score: 0.036\n",
      "sky        â†’ noun-score: 0.035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed()\n",
    "# Corpus\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"many birds fly in the sky\",\n",
    "    \"children play in the garden\",\n",
    "    \"some dogs run fast\",\n",
    "    \"the girl jumped high\",\n",
    "    \"birds sing in trees\",\n",
    "    \"the children run fast\",\n",
    "    \"many children also play\",\n",
    "    \"the dog play with the children\",\n",
    "    \"the birds are on the trees\"\n",
    "]\n",
    "\n",
    "# Tokenize corpus\n",
    "corpus = [w.lower() for s in sentences for w in s.split()]\n",
    "vocab = list(set(corpus))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "# Initialize nouniness score per word\n",
    "noun_score = np.random.randn(V) * 0.1\n",
    "\n",
    "# Transition weight for predicting previous word given nouniness of current\n",
    "transition = np.random.randn(V) * 0.1\n",
    "\n",
    "# Sigmoid and softmax\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Training loop\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "best_loss=None\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for t in range(1, len(corpus)):\n",
    "        curr_word = corpus[t]\n",
    "        prev_word = corpus[t - 1]\n",
    "        i = word2idx[curr_word]\n",
    "        j = word2idx[prev_word]\n",
    "\n",
    "        # Predict previous word distribution using current word's nouniness\n",
    "        pred_scores = noun_score[i] * transition\n",
    "        pred_probs = softmax(pred_scores)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        loss = -np.log(pred_probs[j] + 1e-9)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Gradients\n",
    "        grad = pred_probs\n",
    "        grad[j] -= 1\n",
    "\n",
    "        # Update transition weights\n",
    "        transition -= lr * noun_score[i] * grad\n",
    "\n",
    "        # Update noun_score of current word\n",
    "        noun_score[i] -= lr * np.dot(grad, transition)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "    if best_loss==None: best_loss=total_loss\n",
    "    else:\n",
    "        if total_loss>best_loss: break\n",
    "\n",
    "# Output noun scores\n",
    "print(\"\\nðŸ” Inferred 'nouniness' of words (based on preceding word prediction):\")\n",
    "for i in np.argsort(-sigmoid(noun_score)):\n",
    "    print(f\"{idx2word[i]:10s} â†’ noun-score: {sigmoid(noun_score[i]):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70b8f8d-8ef7-41b3-817c-ba457fad4726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast       -> noun-likelihood: 0.37\n",
      "children   -> noun-likelihood: 1.00\n",
      "a          -> noun-likelihood: 0.73\n",
      "sing       -> noun-likelihood: 1.00\n",
      "jumped     -> noun-likelihood: 1.00\n",
      "on         -> noun-likelihood: 1.00\n",
      "are        -> noun-likelihood: 1.00\n",
      "also       -> noun-likelihood: 1.00\n",
      "some       -> noun-likelihood: 0.60\n",
      "barked     -> noun-likelihood: 1.00\n",
      "garden     -> noun-likelihood: 0.02\n",
      "with       -> noun-likelihood: 1.00\n",
      "sat        -> noun-likelihood: 1.00\n",
      "many       -> noun-likelihood: 0.21\n",
      "girl       -> noun-likelihood: 1.00\n",
      "play       -> noun-likelihood: 1.00\n",
      "high       -> noun-likelihood: 0.30\n",
      "the        -> noun-likelihood: 1.00\n",
      "in         -> noun-likelihood: 1.00\n",
      "cat        -> noun-likelihood: 1.00\n",
      "mat        -> noun-likelihood: 0.61\n",
      "birds      -> noun-likelihood: 1.00\n",
      "dogs       -> noun-likelihood: 1.00\n",
      "run        -> noun-likelihood: 1.00\n",
      "dog        -> noun-likelihood: 1.00\n",
      "loudly     -> noun-likelihood: 0.79\n",
      "sky        -> noun-likelihood: 0.20\n",
      "trees      -> noun-likelihood: 0.51\n",
      "fly        -> noun-likelihood: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Example corpus\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the man saw a woman\",\n",
    "    \"birds fly over trees\",\n",
    "    \"she eats quickly\"\n",
    "]\n",
    "\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"many birds fly in the sky\",\n",
    "    \"children play in the garden\",\n",
    "    \"some dogs run fast\",\n",
    "    \"the girl jumped high\",\n",
    "    \"birds sing in trees\",\n",
    "    \"the children run fast\",\n",
    "    \"many children also play\",\n",
    "    \"the dog play with the children\",\n",
    "    \"the birds are on the trees\"\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "tokenized = [s.lower().split() for s in sentences]\n",
    "vocab = list(set(word for sent in tokenized for word in sent))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize noun scores randomly [0,1]\n",
    "np.random.seed(42)\n",
    "noun_scores = np.random.rand(vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "\n",
    "# Helper to get context counts\n",
    "def get_contexts(tokenized):\n",
    "    contexts = []\n",
    "    for sent in tokenized:\n",
    "        for i in range(1, len(sent)-1):\n",
    "            center = sent[i]\n",
    "            prev_w = sent[i-1]\n",
    "            next_w = sent[i+1]\n",
    "            contexts.append((center, prev_w, next_w))\n",
    "    return contexts\n",
    "\n",
    "contexts = get_contexts(tokenized)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Collect context predictions\n",
    "    pred_prev = defaultdict(float)\n",
    "    pred_next = defaultdict(float)\n",
    "    count_prev = defaultdict(float)\n",
    "    count_next = defaultdict(float)\n",
    "\n",
    "    # E-step: build context prediction tables\n",
    "    for center, prev, next_ in contexts:\n",
    "        idx = word2idx[center]\n",
    "        weight = noun_scores[idx]\n",
    "        pred_prev[prev] += weight\n",
    "        pred_next[next_] += weight\n",
    "        count_prev[prev] += 1\n",
    "        count_next[next_] += 1\n",
    "\n",
    "    # Normalize\n",
    "    for word in pred_prev:\n",
    "        pred_prev[word] /= (count_prev[word] + 1e-5)\n",
    "    for word in pred_next:\n",
    "        pred_next[word] /= (count_next[word] + 1e-5)\n",
    "\n",
    "    # M-step: update noun scores\n",
    "    for center, prev, next_ in contexts:\n",
    "        idx = word2idx[center]\n",
    "        score = noun_scores[idx]\n",
    "\n",
    "        # How well does this noun score predict actual context?\n",
    "        pred_quality = pred_prev[prev] + pred_next[next_]\n",
    "\n",
    "        # Gradient-like update\n",
    "        noun_scores[idx] += lr * (pred_quality - score)\n",
    "        noun_scores[idx] = np.clip(noun_scores[idx], 0, 1)\n",
    "\n",
    "# Show learned noun scores\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word:10} -> noun-likelihood: {noun_scores[idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5defddba-f294-4429-95ed-70c6044f01ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 121.9687\n",
      "Epoch 51, Loss: 121.5021\n",
      "Epoch 101, Loss: 119.1262\n",
      "Epoch 151, Loss: 107.3537\n",
      "Epoch 201, Loss: 97.3362\n",
      "Epoch 251, Loss: 89.7385\n",
      "Epoch 301, Loss: 86.2087\n",
      "Epoch 351, Loss: 84.2420\n",
      "Epoch 401, Loss: 82.8808\n",
      "Epoch 451, Loss: 81.9030\n",
      "Epoch 501, Loss: 81.1513\n",
      "Epoch 551, Loss: 80.5200\n",
      "Epoch 601, Loss: 79.9669\n",
      "Epoch 651, Loss: 79.4756\n",
      "Epoch 701, Loss: 79.0371\n",
      "Epoch 751, Loss: 78.6447\n",
      "Epoch 801, Loss: 78.2922\n",
      "Epoch 851, Loss: 77.9741\n",
      "Epoch 901, Loss: 77.6859\n",
      "Epoch 951, Loss: 77.4236\n",
      "Epoch 1001, Loss: 77.1838\n",
      "Epoch 1051, Loss: 76.9636\n",
      "Epoch 1101, Loss: 76.7601\n",
      "Epoch 1151, Loss: 76.5714\n",
      "Epoch 1201, Loss: 76.3954\n",
      "Epoch 1251, Loss: 76.2306\n",
      "Epoch 1301, Loss: 76.0758\n",
      "Epoch 1351, Loss: 75.9299\n",
      "Epoch 1401, Loss: 75.7920\n",
      "Epoch 1451, Loss: 75.6613\n",
      "Epoch 1501, Loss: 75.5373\n",
      "Epoch 1551, Loss: 75.4195\n",
      "Epoch 1601, Loss: 75.3074\n",
      "Epoch 1651, Loss: 75.2006\n",
      "Epoch 1701, Loss: 75.0987\n",
      "Epoch 1751, Loss: 75.0014\n",
      "Epoch 1801, Loss: 74.9085\n",
      "Epoch 1851, Loss: 74.8196\n",
      "Epoch 1901, Loss: 74.7345\n",
      "Epoch 1951, Loss: 74.6529\n",
      "Epoch 2000, Loss: 74.5763\n",
      "\n",
      "ðŸ” Inferred 'verbiness' of words (based on preceding word prediction):\n",
      "house      â†’ verb-score: 0.987\n",
      "sky        â†’ verb-score: 0.986\n",
      "garden     â†’ verb-score: 0.986\n",
      "mat        â†’ verb-score: 0.986\n",
      "girl       â†’ verb-score: 0.986\n",
      "cat        â†’ verb-score: 0.986\n",
      "trees      â†’ verb-score: 0.866\n",
      "the        â†’ verb-score: 0.788\n",
      "in         â†’ verb-score: 0.656\n",
      "on         â†’ verb-score: 0.586\n",
      "dog        â†’ verb-score: 0.585\n",
      "many       â†’ verb-score: 0.571\n",
      "high       â†’ verb-score: 0.550\n",
      "a          â†’ verb-score: 0.547\n",
      "fast       â†’ verb-score: 0.482\n",
      "children   â†’ verb-score: 0.462\n",
      "some       â†’ verb-score: 0.458\n",
      "barked     â†’ verb-score: 0.376\n",
      "sat        â†’ verb-score: 0.353\n",
      "loudly     â†’ verb-score: 0.320\n",
      "birds      â†’ verb-score: 0.318\n",
      "dogs       â†’ verb-score: 0.316\n",
      "jumped     â†’ verb-score: 0.314\n",
      "play       â†’ verb-score: 0.017\n",
      "sing       â†’ verb-score: 0.009\n",
      "fly        â†’ verb-score: 0.009\n",
      "run        â†’ verb-score: 0.009\n"
     ]
    }
   ],
   "source": [
    "#infer verb weights of words\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed()\n",
    "\n",
    "\n",
    "# Corpus: tokenized and lowercased\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"many birds fly in the sky\",\n",
    "    \"children play in the garden\",\n",
    "    \"some dogs run fast\",\n",
    "    \"the girl jumped high\",\n",
    "    \"birds sing in trees\",\n",
    "    \"dogs play in the house\"\n",
    "]\n",
    "\n",
    "# Flattened corpus\n",
    "corpus = [w.lower() for s in sentences for w in s.split()]\n",
    "vocab = list(set(corpus))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "# Initialize verbiness score per word\n",
    "verb_score = np.random.randn(V) * 0.1\n",
    "\n",
    "# Transition vector: how verb_score helps predict previous word\n",
    "transition = np.random.randn(V) * 0.1\n",
    "\n",
    "# Sigmoid and softmax\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Training loop\n",
    "lr = 0.01\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for t in range(1, len(corpus)):\n",
    "        curr_word = corpus[t]\n",
    "        prev_word = corpus[t - 1]\n",
    "        i = word2idx[curr_word]\n",
    "        j = word2idx[prev_word]\n",
    "\n",
    "        # Predict previous word using verb_score\n",
    "        pred_scores = verb_score[i] * transition\n",
    "        pred_probs = softmax(pred_scores)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        loss = -np.log(pred_probs[j] + 1e-9)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Gradients\n",
    "        grad = pred_probs\n",
    "        grad[j] -= 1\n",
    "\n",
    "        # Update transition vector\n",
    "        transition -= lr * verb_score[i] * grad\n",
    "\n",
    "        # Update verb_score\n",
    "        verb_score[i] -= lr * np.dot(grad, transition)\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Output verb scores\n",
    "print(\"\\nðŸ” Inferred 'verbiness' of words (based on preceding word prediction):\")\n",
    "for i in np.argsort(-sigmoid(verb_score)):\n",
    "    print(f\"{idx2word[i]:10s} â†’ verb-score: {sigmoid(verb_score[i]):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297044c5-8d2a-4e7b-ba52-6b184d4753f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/200 complete.\n",
      "Iteration 2/200 complete.\n",
      "Iteration 3/200 complete.\n",
      "Iteration 4/200 complete.\n",
      "Iteration 5/200 complete.\n",
      "Iteration 6/200 complete.\n",
      "Iteration 7/200 complete.\n",
      "Iteration 8/200 complete.\n",
      "Iteration 9/200 complete.\n",
      "Iteration 10/200 complete.\n",
      "Iteration 11/200 complete.\n",
      "Iteration 12/200 complete.\n",
      "Iteration 13/200 complete.\n",
      "Iteration 14/200 complete.\n",
      "Iteration 15/200 complete.\n",
      "Iteration 16/200 complete.\n",
      "Iteration 17/200 complete.\n",
      "Iteration 18/200 complete.\n",
      "Iteration 19/200 complete.\n",
      "Iteration 20/200 complete.\n",
      "Iteration 21/200 complete.\n",
      "Iteration 22/200 complete.\n",
      "Iteration 23/200 complete.\n",
      "Iteration 24/200 complete.\n",
      "Iteration 25/200 complete.\n",
      "Iteration 26/200 complete.\n",
      "Iteration 27/200 complete.\n",
      "Iteration 28/200 complete.\n",
      "Iteration 29/200 complete.\n",
      "Iteration 30/200 complete.\n",
      "Iteration 31/200 complete.\n",
      "Iteration 32/200 complete.\n",
      "Iteration 33/200 complete.\n",
      "Iteration 34/200 complete.\n",
      "Iteration 35/200 complete.\n",
      "Iteration 36/200 complete.\n",
      "Iteration 37/200 complete.\n",
      "Iteration 38/200 complete.\n",
      "Iteration 39/200 complete.\n",
      "Iteration 40/200 complete.\n",
      "Iteration 41/200 complete.\n",
      "Iteration 42/200 complete.\n",
      "Iteration 43/200 complete.\n",
      "Iteration 44/200 complete.\n",
      "Iteration 45/200 complete.\n",
      "Iteration 46/200 complete.\n",
      "Iteration 47/200 complete.\n",
      "Iteration 48/200 complete.\n",
      "Iteration 49/200 complete.\n",
      "Iteration 50/200 complete.\n",
      "Iteration 51/200 complete.\n",
      "Iteration 52/200 complete.\n",
      "Iteration 53/200 complete.\n",
      "Iteration 54/200 complete.\n",
      "Iteration 55/200 complete.\n",
      "Iteration 56/200 complete.\n",
      "Iteration 57/200 complete.\n",
      "Iteration 58/200 complete.\n",
      "Iteration 59/200 complete.\n",
      "Iteration 60/200 complete.\n",
      "Iteration 61/200 complete.\n",
      "Iteration 62/200 complete.\n",
      "Iteration 63/200 complete.\n",
      "Iteration 64/200 complete.\n",
      "Iteration 65/200 complete.\n",
      "Iteration 66/200 complete.\n",
      "Iteration 67/200 complete.\n",
      "Iteration 68/200 complete.\n",
      "Iteration 69/200 complete.\n",
      "Iteration 70/200 complete.\n",
      "Iteration 71/200 complete.\n",
      "Iteration 72/200 complete.\n",
      "Iteration 73/200 complete.\n",
      "Iteration 74/200 complete.\n",
      "Iteration 75/200 complete.\n",
      "Iteration 76/200 complete.\n",
      "Iteration 77/200 complete.\n",
      "Iteration 78/200 complete.\n",
      "Iteration 79/200 complete.\n",
      "Iteration 80/200 complete.\n",
      "Iteration 81/200 complete.\n",
      "Iteration 82/200 complete.\n",
      "Iteration 83/200 complete.\n",
      "Iteration 84/200 complete.\n",
      "Iteration 85/200 complete.\n",
      "Iteration 86/200 complete.\n",
      "Iteration 87/200 complete.\n",
      "Iteration 88/200 complete.\n",
      "Iteration 89/200 complete.\n",
      "Iteration 90/200 complete.\n",
      "Iteration 91/200 complete.\n",
      "Iteration 92/200 complete.\n",
      "Iteration 93/200 complete.\n",
      "Iteration 94/200 complete.\n",
      "Iteration 95/200 complete.\n",
      "Iteration 96/200 complete.\n",
      "Iteration 97/200 complete.\n",
      "Iteration 98/200 complete.\n",
      "Iteration 99/200 complete.\n",
      "Iteration 100/200 complete.\n",
      "Iteration 101/200 complete.\n",
      "Iteration 102/200 complete.\n",
      "Iteration 103/200 complete.\n",
      "Iteration 104/200 complete.\n",
      "Iteration 105/200 complete.\n",
      "Iteration 106/200 complete.\n",
      "Iteration 107/200 complete.\n",
      "Iteration 108/200 complete.\n",
      "Iteration 109/200 complete.\n",
      "Iteration 110/200 complete.\n",
      "Iteration 111/200 complete.\n",
      "Iteration 112/200 complete.\n",
      "Iteration 113/200 complete.\n",
      "Iteration 114/200 complete.\n",
      "Iteration 115/200 complete.\n",
      "Iteration 116/200 complete.\n",
      "Iteration 117/200 complete.\n",
      "Iteration 118/200 complete.\n",
      "Iteration 119/200 complete.\n",
      "Iteration 120/200 complete.\n",
      "Iteration 121/200 complete.\n",
      "Iteration 122/200 complete.\n",
      "Iteration 123/200 complete.\n",
      "Iteration 124/200 complete.\n",
      "Iteration 125/200 complete.\n",
      "Iteration 126/200 complete.\n",
      "Iteration 127/200 complete.\n",
      "Iteration 128/200 complete.\n",
      "Iteration 129/200 complete.\n",
      "Iteration 130/200 complete.\n",
      "Iteration 131/200 complete.\n",
      "Iteration 132/200 complete.\n",
      "Iteration 133/200 complete.\n",
      "Iteration 134/200 complete.\n",
      "Iteration 135/200 complete.\n",
      "Iteration 136/200 complete.\n",
      "Iteration 137/200 complete.\n",
      "Iteration 138/200 complete.\n",
      "Iteration 139/200 complete.\n",
      "Iteration 140/200 complete.\n",
      "Iteration 141/200 complete.\n",
      "Iteration 142/200 complete.\n",
      "Iteration 143/200 complete.\n",
      "Iteration 144/200 complete.\n",
      "Iteration 145/200 complete.\n",
      "Iteration 146/200 complete.\n",
      "Iteration 147/200 complete.\n",
      "Iteration 148/200 complete.\n",
      "Iteration 149/200 complete.\n",
      "Iteration 150/200 complete.\n",
      "Iteration 151/200 complete.\n",
      "Iteration 152/200 complete.\n",
      "Iteration 153/200 complete.\n",
      "Iteration 154/200 complete.\n",
      "Iteration 155/200 complete.\n",
      "Iteration 156/200 complete.\n",
      "Iteration 157/200 complete.\n",
      "Iteration 158/200 complete.\n",
      "Iteration 159/200 complete.\n",
      "Iteration 160/200 complete.\n",
      "Iteration 161/200 complete.\n",
      "Iteration 162/200 complete.\n",
      "Iteration 163/200 complete.\n",
      "Iteration 164/200 complete.\n",
      "Iteration 165/200 complete.\n",
      "Iteration 166/200 complete.\n",
      "Iteration 167/200 complete.\n",
      "Iteration 168/200 complete.\n",
      "Iteration 169/200 complete.\n",
      "Iteration 170/200 complete.\n",
      "Iteration 171/200 complete.\n",
      "Iteration 172/200 complete.\n",
      "Iteration 173/200 complete.\n",
      "Iteration 174/200 complete.\n",
      "Iteration 175/200 complete.\n",
      "Iteration 176/200 complete.\n",
      "Iteration 177/200 complete.\n",
      "Iteration 178/200 complete.\n",
      "Iteration 179/200 complete.\n",
      "Iteration 180/200 complete.\n",
      "Iteration 181/200 complete.\n",
      "Iteration 182/200 complete.\n",
      "Iteration 183/200 complete.\n",
      "Iteration 184/200 complete.\n",
      "Iteration 185/200 complete.\n",
      "Iteration 186/200 complete.\n",
      "Iteration 187/200 complete.\n",
      "Iteration 188/200 complete.\n",
      "Iteration 189/200 complete.\n",
      "Iteration 190/200 complete.\n",
      "Iteration 191/200 complete.\n",
      "Iteration 192/200 complete.\n",
      "Iteration 193/200 complete.\n",
      "Iteration 194/200 complete.\n",
      "Iteration 195/200 complete.\n",
      "Iteration 196/200 complete.\n",
      "Iteration 197/200 complete.\n",
      "Iteration 198/200 complete.\n",
      "Iteration 199/200 complete.\n",
      "Iteration 200/200 complete.\n",
      "\n",
      "--- Learned Tags ---\n",
      "[('the', 'NOUN'), ('fat', 'PREP'), ('cat', 'ADJ'), ('sat', 'VERB'), ('on', 'NOUN'), ('the', 'PREP'), ('mat', 'ADJ')]\n",
      "[('a', 'DET'), ('dog', 'ADJ'), ('ran', 'VERB'), ('fast', 'NOUN')]\n",
      "[('the', 'DET'), ('birds', 'ADJ'), ('sing', 'VERB')]\n",
      "[('the', 'PREP'), ('big', 'ADJ'), ('bird', 'VERB'), ('sings', 'NOUN')]\n",
      "[('the', 'PREP'), ('cat', 'ADJ'), ('ran', 'VERB'), ('in', 'NOUN'), ('the', 'PREP'), ('house', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class BayesianHMMUnsupervisedPOS:\n",
    "    def __init__(self, possible_tags, vocab, alpha=1.0, beta=1.0, smoothing_factor=0.1):\n",
    "        self.possible_tags = possible_tags  # List of possible POS tags\n",
    "        self.vocab = vocab  # List of all unique words in the corpus\n",
    "        self.alpha = alpha  # Dirichlet prior for transition probabilities\n",
    "        self.beta = beta  # Dirichlet prior for emission probabilities\n",
    "        self.smoothing_factor = smoothing_factor  # For unseen words\n",
    "\n",
    "        # Initialize counts\n",
    "        self.transition_counts = defaultdict(lambda: defaultdict(float)) # P(tag_j | tag_i)\n",
    "        self.emission_counts = defaultdict(lambda: defaultdict(float)) # P(word | tag)\n",
    "        self.tag_counts = defaultdict(float) # P(tag)\n",
    "\n",
    "    def _initialize_random_tags(self, sentences):\n",
    "        # Assign a random tag to each word initially\n",
    "        initial_tags = []\n",
    "        for sentence in sentences:\n",
    "            sentence_tags = []\n",
    "            for word in sentence:\n",
    "                sentence_tags.append(random.choice(self.possible_tags))\n",
    "            initial_tags.append(sentence_tags)\n",
    "        return initial_tags\n",
    "\n",
    "    def _update_counts(self, sentences, tags):\n",
    "        # Reset counts for each iteration\n",
    "        self.transition_counts = defaultdict(lambda: defaultdict(float))\n",
    "        self.emission_counts = defaultdict(lambda: defaultdict(float))\n",
    "        self.tag_counts = defaultdict(float)\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            current_tags = tags[i]\n",
    "            # Handle initial tag (transition from a 'start' state)\n",
    "            if current_tags:\n",
    "                self.tag_counts[current_tags[0]] += 1\n",
    "\n",
    "            for j in range(len(sentence)):\n",
    "                word = sentence[j]\n",
    "                tag = current_tags[j]\n",
    "\n",
    "                self.emission_counts[tag][word] += 1\n",
    "                self.tag_counts[tag] += 1\n",
    "\n",
    "                if j > 0:\n",
    "                    prev_tag = current_tags[j-1]\n",
    "                    self.transition_counts[prev_tag][tag] += 1\n",
    "\n",
    "    def _calculate_probabilities(self):\n",
    "        # Calculate smoothed probabilities (using Dirichlet priors)\n",
    "\n",
    "        # Emission probabilities: P(word | tag)\n",
    "        emission_probs = defaultdict(lambda: defaultdict(float))\n",
    "        for tag in self.possible_tags:\n",
    "            for word in self.vocab:\n",
    "                # Add beta prior (smoothing)\n",
    "                numerator = self.emission_counts[tag][word] + self.beta\n",
    "                denominator = self.tag_counts[tag] + self.beta * len(self.vocab)\n",
    "                emission_probs[tag][word] = numerator / denominator\n",
    "\n",
    "        # Transition probabilities: P(tag_j | tag_i)\n",
    "        transition_probs = defaultdict(lambda: defaultdict(float))\n",
    "        for prev_tag in self.possible_tags:\n",
    "            for current_tag in self.possible_tags:\n",
    "                # Add alpha prior (smoothing)\n",
    "                numerator = self.transition_counts[prev_tag][current_tag] + self.alpha\n",
    "                denominator = sum(self.transition_counts[prev_tag].values()) + self.alpha * len(self.possible_tags)\n",
    "                transition_probs[prev_tag][current_tag] = numerator / denominator\n",
    "\n",
    "        return emission_probs, transition_probs\n",
    "\n",
    "    def _gibbs_sample_tag(self, sentence, current_tags, word_index, emission_probs, transition_probs):\n",
    "        word = sentence[word_index]\n",
    "        probabilities = []\n",
    "\n",
    "        for candidate_tag in self.possible_tags:\n",
    "            # Emission probability\n",
    "            emission_p = emission_probs[candidate_tag][word]\n",
    "\n",
    "            # Transition probability from previous tag\n",
    "            prev_tag = current_tags[word_index - 1] if word_index > 0 else None\n",
    "            if prev_tag:\n",
    "                transition_p_from_prev = transition_probs[prev_tag][candidate_tag]\n",
    "            else: # If it's the first word, no previous tag\n",
    "                transition_p_from_prev = 1.0 # Or some initial tag probability\n",
    "\n",
    "            # Transition probability to next tag\n",
    "            next_tag = current_tags[word_index + 1] if word_index < len(sentence) - 1 else None\n",
    "            if next_tag:\n",
    "                transition_p_to_next = transition_probs[candidate_tag][next_tag]\n",
    "            else: # If it's the last word, no next tag\n",
    "                transition_p_to_next = 1.0 # Or some final tag probability\n",
    "\n",
    "            # Calculate joint probability (proportional to posterior)\n",
    "            prob = emission_p * transition_p_from_prev * transition_p_to_next\n",
    "            probabilities.append(prob)\n",
    "\n",
    "        # Normalize and sample a new tag\n",
    "        total_prob = sum(probabilities)\n",
    "        if total_prob == 0:  # Handle cases where all probabilities are zero\n",
    "            # Fallback: assign a random tag or use a default\n",
    "            return random.choice(self.possible_tags)\n",
    "\n",
    "        normalized_probs = [p / total_prob for p in probabilities]\n",
    "        new_tag = random.choices(self.possible_tags, weights=normalized_probs, k=1)[0]\n",
    "        return new_tag\n",
    "\n",
    "    def train(self, sentences, iterations=50):\n",
    "        # 1. Initialize tags randomly\n",
    "        current_tags = self._initialize_random_tags(sentences)\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            # 2. Update counts based on current tags\n",
    "            self._update_counts(sentences, current_tags)\n",
    "\n",
    "            # 3. Calculate (smoothed) probabilities\n",
    "            emission_probs, transition_probs = self._calculate_probabilities()\n",
    "\n",
    "            # 4. Gibbs sample new tags for each word\n",
    "            for s_idx, sentence in enumerate(sentences):\n",
    "                for w_idx in range(len(sentence)):\n",
    "                    new_tag = self._gibbs_sample_tag(\n",
    "                        sentence,\n",
    "                        current_tags[s_idx],\n",
    "                        w_idx,\n",
    "                        emission_probs,\n",
    "                        transition_probs\n",
    "                    )\n",
    "                    current_tags[s_idx][w_idx] = new_tag\n",
    "\n",
    "            print(f\"Iteration {iteration + 1}/{iterations} complete.\")\n",
    "\n",
    "        return current_tags\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Dummy data - in a real scenario, you'd have a large corpus\n",
    "corpus = [\n",
    "    [\"the\", \"fat\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"a\", \"dog\", \"ran\", \"fast\"],\n",
    "    [\"the\", \"birds\", \"sing\"],\n",
    "    [\"the\", \"big\",\"bird\",  \"sings\"],\n",
    "    [\"the\", \"cat\", \"ran\",\"in\",\"the\",\"house\"]\n",
    "]\n",
    "\n",
    "# Define possible tags (simplified for illustration)\n",
    "possible_tags = [\"DET\", \"NOUN\", \"VERB\", \"PREP\", \"ADJ\"]\n",
    "\n",
    "# Build vocabulary from the corpus\n",
    "all_words = set(word for sentence in corpus for word in sentence)\n",
    "vocab = list(all_words)\n",
    "\n",
    "# Initialize and train the model\n",
    "b_hmm = BayesianHMMUnsupervisedPOS(possible_tags, vocab, alpha=0.1, beta=0.1) # Small Dirichlet priors\n",
    "learned_tags = b_hmm.train(corpus, iterations=200)\n",
    "\n",
    "# Print the tagged sentences\n",
    "print(\"\\n--- Learned Tags ---\")\n",
    "for i, sentence in enumerate(corpus):\n",
    "    tagged_sentence = list(zip(sentence, learned_tags[i]))\n",
    "    print(tagged_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffa1d06-87fb-47b1-b13e-4b8868b36505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 0), ('cat', 2), ('sat', 1), ('on', 4), ('the', 0), ('mat', 1)],\n",
       " [('a', 1), ('dog', 3), ('barked', 2), ('loudly', 1)],\n",
       " [('the', 1), ('man', 4), ('saw', 0), ('a', 1), ('woman', 4)],\n",
       " [('birds', 0), ('fly', 1), ('over', 3), ('trees', 2)],\n",
       " [('she', 1), ('eats', 4), ('quickly', 0)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Example corpus\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"the man saw a woman\",\n",
    "    \"birds fly over trees\",\n",
    "    \"she eats quickly\"\n",
    "]\n",
    "\n",
    "# Tokenize and build vocabulary\n",
    "tokenized = [s.lower().split() for s in sentences]\n",
    "vocab = list(set(word for sent in tokenized for word in sent))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Parameters\n",
    "num_tags = 5  # number of hidden states (e.g., POS tags)\n",
    "vocab_size = len(vocab)\n",
    "num_sentences = len(tokenized)\n",
    "\n",
    "# Initialize HMM parameters randomly\n",
    "np.random.seed(42)\n",
    "trans_probs = np.random.dirichlet(np.ones(num_tags), size=num_tags)        # P(tag_t | tag_{t-1})\n",
    "emit_probs = np.random.dirichlet(np.ones(vocab_size), size=num_tags)       # P(word | tag)\n",
    "start_probs = np.random.dirichlet(np.ones(num_tags))                       # P(tag_0)\n",
    "\n",
    "# Forward-Backward (E-step) and parameter update (M-step)\n",
    "def forward(sent):\n",
    "    T = len(sent)\n",
    "    alpha = np.zeros((T, num_tags))\n",
    "    alpha[0] = start_probs * emit_probs[:, sent[0]]\n",
    "    for t in range(1, T):\n",
    "        for j in range(num_tags):\n",
    "            alpha[t, j] = np.sum(alpha[t-1] * trans_probs[:, j]) * emit_probs[j, sent[t]]\n",
    "    return alpha\n",
    "\n",
    "def backward(sent):\n",
    "    T = len(sent)\n",
    "    beta = np.zeros((T, num_tags))\n",
    "    beta[T-1] = 1\n",
    "    for t in reversed(range(T-1)):\n",
    "        for i in range(num_tags):\n",
    "            beta[t, i] = np.sum(beta[t+1] * trans_probs[i] * emit_probs[:, sent[t+1]])\n",
    "    return beta\n",
    "\n",
    "def baum_welch(tokenized_idx, epochs=10):\n",
    "    global trans_probs, emit_probs, start_probs\n",
    "    for epoch in range(epochs):\n",
    "        A = np.zeros_like(trans_probs)\n",
    "        B = np.zeros_like(emit_probs)\n",
    "        pi = np.zeros(num_tags)\n",
    "        \n",
    "        for sent in tokenized_idx:\n",
    "            T = len(sent)\n",
    "            alpha = forward(sent)\n",
    "            beta = backward(sent)\n",
    "            prob = np.sum(alpha[-1])\n",
    "\n",
    "            gamma = (alpha * beta) / prob\n",
    "            xi = np.zeros((T-1, num_tags, num_tags))\n",
    "\n",
    "            for t in range(T-1):\n",
    "                denom = np.sum(alpha[t][:, None] * trans_probs * emit_probs[:, sent[t+1]] * beta[t+1])\n",
    "                for i in range(num_tags):\n",
    "                    for j in range(num_tags):\n",
    "                        xi[t, i, j] = alpha[t, i] * trans_probs[i, j] * emit_probs[j, sent[t+1]] * beta[t+1, j]\n",
    "                xi[t] /= denom\n",
    "\n",
    "            pi += gamma[0]\n",
    "            for t in range(T):\n",
    "                B[:, sent[t]] += gamma[t]\n",
    "            for t in range(T-1):\n",
    "                A += xi[t]\n",
    "        \n",
    "        # Normalize\n",
    "        trans_probs = A / A.sum(axis=1, keepdims=True)\n",
    "        emit_probs = B / B.sum(axis=1, keepdims=True)\n",
    "        start_probs = pi / pi.sum()\n",
    "\n",
    "# Convert tokenized to indices\n",
    "tokenized_idx = [[word2idx[w] for w in sent] for sent in tokenized]\n",
    "\n",
    "# Run training\n",
    "baum_welch(tokenized_idx, epochs=10)\n",
    "\n",
    "# Viterbi decoding\n",
    "def viterbi(sent):\n",
    "    T = len(sent)\n",
    "    delta = np.zeros((T, num_tags))\n",
    "    psi = np.zeros((T, num_tags), dtype=int)\n",
    "    \n",
    "    delta[0] = start_probs * emit_probs[:, sent[0]]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        for j in range(num_tags):\n",
    "            scores = delta[t-1] * trans_probs[:, j]\n",
    "            psi[t, j] = np.argmax(scores)\n",
    "            delta[t, j] = np.max(scores) * emit_probs[j, sent[t]]\n",
    "    \n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[-1] = np.argmax(delta[-1])\n",
    "    for t in reversed(range(1, T)):\n",
    "        states[t-1] = psi[t, states[t]]\n",
    "    \n",
    "    return states\n",
    "\n",
    "# Predict tags\n",
    "tagged_sentences = []\n",
    "for sent in tokenized_idx:\n",
    "    tags = viterbi(sent)\n",
    "    tags=[int(v) for v in tags]\n",
    "    tagged_sentences.append(list(zip([idx2word[i]  for i in sent], tags)))\n",
    "\n",
    "tagged_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a8b5b21-e6af-41f6-a79a-567852a89e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'an': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'apple': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'ball': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'barked': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'birds': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'books': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'boy': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'cat': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'children': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'dog': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'door': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'eats': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'fast': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'fly': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'he': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'in': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'like': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'man': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'mat': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'on': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'opened': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'outside': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'play': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'plays': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'read': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'runs': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'sat': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'she': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'shines': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'sky': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'sun': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'the': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'they': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'to': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25},\n",
       " 'with': {'NOUN': 0.25, 'VERB': 0.25, 'DET': 0.25, 'OTHER': 0.25}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's expand the corpus with more diverse and structured sentences\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"barked\"],\n",
    "    [\"she\", \"eats\", \"an\", \"apple\"],\n",
    "    [\"he\", \"runs\", \"fast\"],\n",
    "    [\"a\", \"boy\", \"plays\", \"with\", \"a\", \"ball\"],\n",
    "    [\"birds\", \"fly\", \"in\", \"the\", \"sky\"],\n",
    "    [\"the\", \"sun\", \"shines\"],\n",
    "    [\"they\", \"read\", \"books\"],\n",
    "    [\"children\", \"like\", \"to\", \"play\", \"outside\"],\n",
    "    [\"the\", \"man\", \"opened\", \"the\", \"door\"],\n",
    "]\n",
    "\n",
    "# Reuse the same training function from earlier\n",
    "def train_unsupervised_pos_tagger(corpus, tags, num_iterations=10):\n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "\n",
    "    vocab = set(word for sentence in corpus for word in sentence)\n",
    "    word_tag_probs = {word: np.random.dirichlet(np.ones(len(tags))).tolist() for word in vocab}\n",
    "    tag_transition_probs = {t1: {t2: 1.0 / len(tags) for t2 in tags} for t1 in tags}\n",
    "    word_given_tag_probs = {t: defaultdict(lambda: 1e-6) for t in tags}\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        tag_counts = defaultdict(float)\n",
    "        tag_pair_counts = {t1: defaultdict(float) for t1 in tags}\n",
    "        word_tag_counts = {t: defaultdict(float) for t in tags}\n",
    "\n",
    "        for sentence in corpus:\n",
    "            for i, word in enumerate(sentence):\n",
    "                # Get current tag distribution\n",
    "                current_probs = np.array(word_tag_probs[word])\n",
    "                for j, tag in enumerate(tags):\n",
    "                    # Use transition probability from previous tag\n",
    "                    if i > 0:\n",
    "                        prev_word = sentence[i - 1]\n",
    "                        prev_probs = word_tag_probs[prev_word]\n",
    "                        transition_score = sum(prev_probs[k] * tag_transition_probs[tags[k]][tag] for k in range(len(tags)))\n",
    "                    else:\n",
    "                        transition_score = 1.0 / len(tags)\n",
    "                    # Word likelihood\n",
    "                    emission = word_given_tag_probs[tag][word]\n",
    "                    current_probs[j] = transition_score * emission\n",
    "                # Normalize\n",
    "                current_probs /= current_probs.sum() if current_probs.sum() > 0 else 1.0\n",
    "                word_tag_probs[word] = current_probs.tolist()\n",
    "\n",
    "                for j, tag in enumerate(tags):\n",
    "                    tag_counts[tag] += current_probs[j]\n",
    "                    word_tag_counts[tag][word] += current_probs[j]\n",
    "                    if i > 0:\n",
    "                        for k, prev_tag in enumerate(tags):\n",
    "                            tag_pair_counts[prev_tag][tag] += word_tag_probs[sentence[i - 1]][k] * current_probs[j]\n",
    "\n",
    "        # Update transition probabilities\n",
    "        for t1 in tags:\n",
    "            total = sum(tag_pair_counts[t1].values())\n",
    "            for t2 in tags:\n",
    "                tag_transition_probs[t1][t2] = tag_pair_counts[t1][t2] / total if total > 0 else 1.0 / len(tags)\n",
    "\n",
    "        # Update emission probabilities\n",
    "        for tag in tags:\n",
    "            total = sum(word_tag_counts[tag].values())\n",
    "            for word in vocab:\n",
    "                word_given_tag_probs[tag][word] = word_tag_counts[tag][word] / total if total > 0 else 1e-6\n",
    "\n",
    "    # Final tag distributions\n",
    "    return {\n",
    "        word: {tag: round(prob, 3) for tag, prob in zip(tags, probs)}\n",
    "        for word, probs in word_tag_probs.items()\n",
    "    }\n",
    "\n",
    "# Define tags\n",
    "tags = [\"NOUN\", \"VERB\", \"DET\", \"OTHER\"]\n",
    "\n",
    "# Train the model\n",
    "tag_distributions = train_unsupervised_pos_tagger(corpus, tags, num_iterations=10)\n",
    "tag_distributions_sorted = dict(sorted(tag_distributions.items()))\n",
    "\n",
    "tag_distributions_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "732e24a7-2d52-4925-b514-6b0dcdd943d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '<s>', 'a', 'also', 'are', 'barked', 'birds', 'cat', 'children', 'dog', 'dogs', 'fast', 'fly', 'garden', 'girl', 'high', 'house', 'in', 'it', 'jumped', 'loudly', 'many', 'mat', 'on', 'play', 'run', 'sat', 'she', 'sing', 'sky', 'some', 'table', 'the', 'they', 'trees', 'with']\n",
      "P_t_w_array (36, 7)\n",
      "P_w_t_array (7, 36)\n",
      "P_t2_t1_array (7, 7)\n",
      "('the', 'mat')\n",
      "P_w1 (36,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "prediction (36,)\n",
      "[0.20394918 0.         0.01347651 0.0116274  0.00992445 0.01347651\n",
      " 0.03492903 0.01347651 0.04788678 0.02155185 0.00992445 0.0251039\n",
      " 0.01285843 0.01285843 0.01347651 0.02412968 0.00992445 0.04397859\n",
      " 0.01285843 0.02536072 0.00992445 0.02571686 0.01347651 0.03317924\n",
      " 0.05266988 0.0251039  0.0251039  0.01250229 0.01347651 0.01250229\n",
      " 0.01250229 0.01285843 0.14000897 0.01285843 0.02571686 0.0116274 ]\n",
      "actual (36,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#28 July 2025 - step by step initialize & update language model with arbitrary tags, constrained by word tag weights and tag transitions\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import random\n",
    "import math, re\n",
    "\n",
    "def tok_basic(txt,add_sent_tags=False): \n",
    "  txt=re.sub(\"(?u)(\\W)\",r\" \\1 \", txt)\n",
    "  out=re.split(\"\\s+\",txt)\n",
    "  tokens=[v for v in out if v]\n",
    "  if add_sent_tags: tokens=[\"<s>\"]+tokens+[\"</s>\"]\n",
    "  return tokens\n",
    "\n",
    "def create_one_hot_vec(hot_i,vec_size):\n",
    "  zeros=[0.]*vec_size\n",
    "  zeros[hot_i]=1.\n",
    "  return np.array(zeros)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed()\n",
    "# Corpus\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"many birds fly in the sky\",\n",
    "    \"children play in the garden\",\n",
    "    \"some dogs run fast\",\n",
    "    \"the girl jumped high\",\n",
    "    \"birds sing in trees\",\n",
    "    \"the children run fast\",\n",
    "    \"many children also play\",\n",
    "    \"the dog play with the children\",\n",
    "    \"the birds are on the trees\",\n",
    "    \"she jumped high\",\n",
    "    \"they play in the house\",\n",
    "    \"it sat on the table\"\n",
    "]\n",
    "\n",
    "#tags=[\"N\",\"V\",\"PREP\",\"DET\",\"ADJ\",\"ADV\", \"<s>\",\"</s>\"]\n",
    "#available_tags=[\"N\",\"V\",\"PREP\",\"DET\",\"ADJ\",\"ADV\"]\n",
    "available_tags=[\"N\",\"V\",\"PREP\",\"DET\",\"OTHER\"]\n",
    "all_tags=available_tags+[\"<s>\",\"</s>\"]\n",
    "\n",
    "\n",
    "all_tokenized=[]\n",
    "all_token_pairs=[]\n",
    "word_counter={}\n",
    "for sent_i,sent0 in enumerate(sentences):\n",
    "    tokens=tok_basic(sent0,add_sent_tags=True)\n",
    "    all_tokenized.append(tokens)\n",
    "    for tk0 in tokens: word_counter[tk0]=word_counter.get(tk0,0)+1\n",
    "    for tk_i0,cur_tk0 in enumerate(tokens[:-1]):\n",
    "        next_tk0=tokens[tk_i0+1]\n",
    "        all_token_pairs.append((cur_tk0,next_tk0))\n",
    "\n",
    "vocab=sorted(list(word_counter.keys())) \n",
    "\n",
    "print(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "\n",
    "#for a in all_token_pairs: print(a)\n",
    "\n",
    "word_tag_list=[]\n",
    "#tag_counter={}\n",
    "# Tokenize corpus\n",
    "for sent_i,sent_tokens in enumerate(all_tokenized):\n",
    "    #print(sent_tokens)\n",
    "    for tk0 in sent_tokens:\n",
    "        if tk0 in [\"<s>\",\"</s>\"]: cur_tag=tk0 \n",
    "        else: cur_tag = random.choice(available_tags)\n",
    "        word_tag_list.append((sent_i,tk0,cur_tag))\n",
    "        #tag_counter[cur_tag]=tag_counter.get(cur_tag,0)+1\n",
    "\n",
    "sorted_by_word=sorted(word_tag_list,key=lambda x:x[1])\n",
    "sorted_by_tag=sorted(word_tag_list,key=lambda x:x[-1])\n",
    "#word_tag_list.sort(key=lambda x:x[1])\n",
    "tag_pairs_list=[]\n",
    "for i0,cur_item0 in enumerate(word_tag_list[:-1]):\n",
    "    next_item0=word_tag_list[i0+1]\n",
    "    if next_item0[0]!=cur_item0[0]: continue\n",
    "    cur_tag_pair=cur_item0[-1],next_item0[-1]\n",
    "    tag_pairs_list.append(cur_tag_pair)\n",
    "    #print(i0,cur_item0,next_item0,cur_tag_pair)\n",
    "\n",
    "# grouped_by_word=[(key,[v[-1] for v in list(group)]) for key,group in groupby(sorted_by_word,lambda x:x[1]) ]\n",
    "# word_tag_wt_dict={}\n",
    "# for w0,w0_tags0 in grouped_by_word:\n",
    "#     cur_count0=len(w0_tags0)\n",
    "#     counted_tags=dict(Counter(w0_tags0))\n",
    "#     for tag0,tag_count0 in counted_tags.items():\n",
    "#         tag_wt0=tag_count0/cur_count0\n",
    "#         word_tag_wt_dict[(w0,tag0)]=tag_wt0\n",
    "\n",
    "#populating P(t|w) - probability of tag given word\n",
    "P_t_w_list=[]\n",
    "\n",
    "grouped_by_word_dict=dict(iter([(key,dict(Counter([v[-1] for v in list(group)]))) for key,group in groupby(sorted_by_word,lambda x:x[1]) ])) \n",
    "for word0 in vocab:\n",
    "    corr_dict0=grouped_by_word_dict.get(word0,{})\n",
    "    cur_count0=sum(corr_dict0.values())\n",
    "    cur_row0=[]\n",
    "    for tag0 in all_tags:\n",
    "        if cur_count0==0: word_tag_wt0=0\n",
    "        else: word_tag_wt0=corr_dict0.get(tag0,0)/cur_count0\n",
    "        cur_row0.append(word_tag_wt0)\n",
    "    #print(word0,cur_count0,corr_dict0,cur_row0)\n",
    "    P_t_w_list.append(cur_row0)\n",
    "\n",
    "P_t_w_array=np.array(P_t_w_list)\n",
    "print(\"P_t_w_array\",P_t_w_array.shape)\n",
    "\n",
    "\n",
    "grouped_by_tag=[(key,[v[1] for v in list(group)]) for key,group in groupby(sorted_by_tag,lambda x:x[-1]) ]\n",
    "tag_word_wt_dict={}\n",
    "for tag0,tag_words0 in grouped_by_tag:\n",
    "    #print(tag0,tag_words0)\n",
    "    cur_count0=len(tag_words0)\n",
    "    counted_words=dict(Counter(tag_words0))\n",
    "    for word0,word_count0 in counted_words.items():\n",
    "        word_wt0=word_count0/cur_count0\n",
    "        tag_word_wt_dict[(tag0,word0)]=word_wt0\n",
    "\n",
    "#populating P(w|t) - probability of word given tag\n",
    "P_w_t_list=[]\n",
    "for tag0 in all_tags:\n",
    "    #print(tag0)\n",
    "    all_word_weights=[]\n",
    "    for word0 in vocab:\n",
    "        cur_pair=(tag0,word0)\n",
    "        cur_wt=tag_word_wt_dict.get(cur_pair,0)\n",
    "        all_word_weights.append(cur_wt)\n",
    "    #print(all_word_weights)\n",
    "    P_w_t_list.append(all_word_weights)\n",
    "        \n",
    "P_w_t_array=np.array(P_w_t_list)        \n",
    "#print(P_w_t_array)\n",
    "print(\"P_w_t_array\", P_w_t_array.shape)\n",
    "\n",
    "#calculate tag transitions\n",
    "tag_transition_wt_dict={}\n",
    "tag_pairs_list_sorted_w1=sorted(tag_pairs_list,key=lambda x:x[0])\n",
    "#print(tag_pairs_list_sorted_w1)\n",
    "tag_pairs_list_grouped_w1=[(key,[v[1] for v in list(group)]) for key,group in groupby(tag_pairs_list_sorted_w1,lambda x:x[0])]\n",
    "for tag0,grp0 in tag_pairs_list_grouped_w1:\n",
    "    cur_dict=dict(Counter(grp0))\n",
    "    cur_count0=len(grp0)\n",
    "    #print(key0,cur_dict)\n",
    "    for tag1,tag_count1 in cur_dict.items():\n",
    "        tag_wt1=tag_count1/cur_count0\n",
    "        tag_transition_wt_dict[(tag0,tag1)]=tag_wt1\n",
    "\n",
    "#populating P(t2|t1) - probability of tag transition\n",
    "P_t2_t1_list=[]\n",
    "for tag0 in all_tags:\n",
    "    #print(tag0)\n",
    "    all_next_tag_weights=[]\n",
    "    for tag1 in all_tags:\n",
    "        cur_pair=(tag0,tag1)\n",
    "        cur_wt=tag_transition_wt_dict.get(cur_pair,0)\n",
    "        all_next_tag_weights.append(cur_wt)\n",
    "    #print(all_word_weights)\n",
    "    P_t2_t1_list.append(all_next_tag_weights)\n",
    "        \n",
    "P_t2_t1_array=np.array(P_t2_t1_list)        \n",
    "#print(P_w_t_array)\n",
    "print(\"P_t2_t1_array\", P_t2_t1_array.shape)\n",
    "\n",
    "cur_pair_i=5\n",
    "cur_pair=all_token_pairs[cur_pair_i]\n",
    "word1,word2=cur_pair\n",
    "print(cur_pair)\n",
    "P_w1=create_one_hot_vec(word2idx[word1] ,len(vocab) )\n",
    "\n",
    "actual_w2=create_one_hot_vec(word2idx[word2] ,len(vocab) )\n",
    "\n",
    "print(\"P_w1\",P_w1.shape)\n",
    "print(P_w1)\n",
    "\n",
    "\n",
    "prediction0=P_w1 @ P_t_w_array @ P_t2_t1_array @ P_w_t_array\n",
    "\n",
    "print(\"prediction\",prediction0.shape)\n",
    "print(prediction0)\n",
    "\n",
    "print(\"actual\",actual_w2.shape)\n",
    "print(actual_w2)\n",
    "\n",
    "# P_w1 (36,)\n",
    "# P_t_w_array (36, 7)\n",
    "# P_w_t_array (7, 36)\n",
    "# P_t2_t1_array (7, 7)\n",
    "\n",
    "\n",
    "# for a,b in tag_transition_wt_dict.items():\n",
    "#     print(a,b)\n",
    "# for a,b in tag_word_wt_dict.items():\n",
    "#     print(a,b)\n",
    "#print(a,len(b), )\n",
    "#corpus = [[\"<s>\"]+ w.lower() for s in sentences for w in s.split()]\n",
    "# vocab = list(set(corpus))\n",
    "# word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "# idx2word = {i: w for w, i in word2idx.items()}\n",
    "# V = len(vocab)\n",
    "\n",
    "# # Initialize nouniness score per word\n",
    "# noun_score = np.random.randn(V) * 0.1\n",
    "\n",
    "# Transition weight for predicting previous word given nouniness of current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27d542bb-a5ff-4c94-9c78-39a4557609b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'the'),\n",
       " ('the', 'cat'),\n",
       " ('cat', 'sat'),\n",
       " ('sat', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'mat'),\n",
       " ('mat', '</s>'),\n",
       " ('<s>', 'a'),\n",
       " ('a', 'dog'),\n",
       " ('dog', 'barked'),\n",
       " ('barked', 'loudly'),\n",
       " ('loudly', '</s>'),\n",
       " ('<s>', 'many'),\n",
       " ('many', 'birds'),\n",
       " ('birds', 'fly'),\n",
       " ('fly', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'sky'),\n",
       " ('sky', '</s>'),\n",
       " ('<s>', 'children'),\n",
       " ('children', 'play'),\n",
       " ('play', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'garden'),\n",
       " ('garden', '</s>'),\n",
       " ('<s>', 'some'),\n",
       " ('some', 'dogs'),\n",
       " ('dogs', 'run'),\n",
       " ('run', 'fast'),\n",
       " ('fast', '</s>'),\n",
       " ('<s>', 'the'),\n",
       " ('the', 'girl'),\n",
       " ('girl', 'jumped'),\n",
       " ('jumped', 'high'),\n",
       " ('high', '</s>'),\n",
       " ('<s>', 'birds'),\n",
       " ('birds', 'sing'),\n",
       " ('sing', 'in'),\n",
       " ('in', 'trees'),\n",
       " ('trees', '</s>'),\n",
       " ('<s>', 'the'),\n",
       " ('the', 'children'),\n",
       " ('children', 'run'),\n",
       " ('run', 'fast'),\n",
       " ('fast', '</s>'),\n",
       " ('<s>', 'many'),\n",
       " ('many', 'children'),\n",
       " ('children', 'also'),\n",
       " ('also', 'play'),\n",
       " ('play', '</s>'),\n",
       " ('<s>', 'the'),\n",
       " ('the', 'dog'),\n",
       " ('dog', 'play'),\n",
       " ('play', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'children'),\n",
       " ('children', '</s>'),\n",
       " ('<s>', 'the'),\n",
       " ('the', 'birds'),\n",
       " ('birds', 'are'),\n",
       " ('are', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'trees'),\n",
       " ('trees', '</s>'),\n",
       " ('<s>', 'she'),\n",
       " ('she', 'jumped'),\n",
       " ('jumped', 'high'),\n",
       " ('high', '</s>'),\n",
       " ('<s>', 'they'),\n",
       " ('they', 'play'),\n",
       " ('play', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'house'),\n",
       " ('house', '</s>'),\n",
       " ('<s>', 'it'),\n",
       " ('it', 'sat'),\n",
       " ('sat', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'table'),\n",
       " ('table', '</s>')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_token_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6994371a-b505-46de-bae3-a47ecedb2e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'the', 'cat', 'sat', 'on', 'the', 'mat', '</s>']\n",
      "['<s>', 'a', 'dog', 'barked', 'loudly', '</s>']\n",
      "['<s>', 'many', 'birds', 'fly', 'in', 'the', 'sky', '</s>']\n",
      "['<s>', 'children', 'play', 'in', 'the', 'garden', '</s>']\n",
      "['<s>', 'some', 'dogs', 'run', 'fast', '</s>']\n",
      "['<s>', 'the', 'girl', 'jumped', 'high', '</s>']\n",
      "['<s>', 'birds', 'sing', 'in', 'trees', '</s>']\n",
      "['<s>', 'the', 'children', 'run', 'fast', '</s>']\n",
      "['<s>', 'many', 'children', 'also', 'play', '</s>']\n",
      "['<s>', 'the', 'dog', 'play', 'with', 'the', 'children', '</s>']\n",
      "['<s>', 'the', 'birds', 'are', 'on', 'the', 'trees', '</s>']\n",
      "['<s>', 'she', 'jumped', 'high', '</s>']\n",
      "['<s>', 'they', 'play', 'in', 'the', 'house', '</s>']\n",
      "['<s>', 'it', 'sat', 'on', 'the', 'table', '</s>']\n",
      "['<s>', 'there', 'were', '15', 'boys', '</s>']\n",
      "['<s>', 'there', 'are', 'also', '12', 'men', ',', '7', 'women', '</s>']\n",
      "('<s>', 'V')\n",
      "('V', 'OTHER')\n",
      "('OTHER', 'V')\n",
      "('V', 'OTHER')\n",
      "('OTHER', 'DET')\n",
      "('DET', 'NUM')\n",
      "('NUM', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'PREP')\n",
      "('PREP', 'OTHER')\n",
      "('OTHER', 'OTHER')\n",
      "('OTHER', 'V')\n",
      "('V', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'V')\n",
      "('V', 'NUM')\n",
      "('NUM', 'OTHER')\n",
      "('OTHER', 'NUM')\n",
      "('NUM', 'PREP')\n",
      "('PREP', 'N')\n",
      "('N', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'PREP')\n",
      "('PREP', 'N')\n",
      "('N', 'NUM')\n",
      "('NUM', 'N')\n",
      "('N', 'V')\n",
      "('V', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'DET')\n",
      "('DET', 'PREP')\n",
      "('PREP', 'V')\n",
      "('V', 'NUM')\n",
      "('NUM', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'PREP')\n",
      "('PREP', 'PUNC')\n",
      "('PUNC', 'V')\n",
      "('V', 'PREP')\n",
      "('PREP', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'N')\n",
      "('N', 'NUM')\n",
      "('NUM', 'DET')\n",
      "('DET', 'PUNC')\n",
      "('PUNC', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'DET')\n",
      "('DET', 'V')\n",
      "('V', 'PUNC')\n",
      "('PUNC', 'NUM')\n",
      "('NUM', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'OTHER')\n",
      "('OTHER', 'PREP')\n",
      "('PREP', 'DET')\n",
      "('DET', 'NUM')\n",
      "('NUM', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'OTHER')\n",
      "('OTHER', 'NUM')\n",
      "('NUM', 'PREP')\n",
      "('PREP', 'PUNC')\n",
      "('PUNC', 'N')\n",
      "('N', 'PREP')\n",
      "('PREP', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'OTHER')\n",
      "('OTHER', 'OTHER')\n",
      "('OTHER', 'V')\n",
      "('V', 'PREP')\n",
      "('PREP', 'NUM')\n",
      "('NUM', 'DET')\n",
      "('DET', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'V')\n",
      "('V', 'PREP')\n",
      "('PREP', 'DET')\n",
      "('DET', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'DET')\n",
      "('DET', 'PUNC')\n",
      "('PUNC', 'V')\n",
      "('V', 'OTHER')\n",
      "('OTHER', 'PUNC')\n",
      "('PUNC', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'V')\n",
      "('V', 'V')\n",
      "('V', 'PUNC')\n",
      "('PUNC', 'PUNC')\n",
      "('PUNC', 'DET')\n",
      "('DET', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'PREP')\n",
      "('PREP', 'OTHER')\n",
      "('OTHER', 'NUM')\n",
      "('NUM', 'V')\n",
      "('V', '</s>')\n",
      "('</s>', '<s>')\n",
      "('<s>', 'V')\n",
      "('V', 'NUM')\n",
      "('NUM', 'N')\n",
      "('N', 'NUM')\n",
      "('NUM', 'PUNC')\n",
      "('PUNC', 'NUM')\n",
      "('NUM', 'NUM')\n",
      "('NUM', 'PUNC')\n",
      "('PUNC', '</s>')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.shadow at 0x18ce990dbe0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9 August 2025 - class for step by step initialize & update language model with arbitrary tags, constrained by word tag weights and tag transitions\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import random\n",
    "import math, re\n",
    "\n",
    "def tok_basic(txt,add_sent_tags=False): \n",
    "  txt=re.sub(\"(?u)(\\W)\",r\" \\1 \", txt)\n",
    "  out=re.split(\"\\s+\",txt)\n",
    "  tokens=[v for v in out if v]\n",
    "  if add_sent_tags: tokens=[\"<s>\"]+tokens+[\"</s>\"]\n",
    "  return tokens\n",
    "\n",
    "def create_one_hot_vec(hot_i,vec_size):\n",
    "  zeros=[0.]*vec_size\n",
    "  zeros[hot_i]=1.\n",
    "  return np.array(zeros)\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed()\n",
    "\n",
    "class shadow:\n",
    "    def __init__(self,sent_list,tag_list,params={}):\n",
    "        self.sent_list=sent_list\n",
    "        self.tag_list=tag_list\n",
    "        self.params={}\n",
    "        self.add_sent_tags=self.params.get(\"add_sent_tags\",True)\n",
    "        extended_tags=[\"<s>\",\"</s>\"] #maybe will also include numbers and punctuation - special characters, hashtags, whatever\n",
    "        if self.add_sent_tags: \n",
    "            self.tag_list+=extended_tags\n",
    "            self.tag_list=sorted(list(set(self.tag_list)))\n",
    "        self.available_tags=[v for v in self.tag_list if not v in extended_tags]\n",
    "        self.tokenized_sent_list=[]\n",
    "        self.tagged_tokens_list=[]\n",
    "        self.token_counter={}\n",
    "        self.ngram_list=[]\n",
    "        for sent0 in self.sent_list:\n",
    "            cur_tokens=tok_basic(sent0,add_sent_tags=self.add_sent_tags)\n",
    "            print(cur_tokens)\n",
    "            cur_tokens_with_tags=[]\n",
    "            for tk0 in cur_tokens: \n",
    "                self.token_counter[tk0]=self.token_counter.get(tk0,0)+1\n",
    "                assigned_tag=self.assign_tag(tk0)\n",
    "                cur_tokens_with_tags.append((tk0,assigned_tag))\n",
    "                self.tagged_tokens_list.append((tk0,assigned_tag))\n",
    "            for tk_i0,cur_tk0 in enumerate(cur_tokens[:-1]):\n",
    "                next_tk0=cur_tokens[tk_i0+1]\n",
    "                self.ngram_list.append((cur_tk0,next_tk0))\n",
    "            # print(cur_tokens_with_tags)\n",
    "            # self.tokenized_tagged_sent_list.append()\n",
    "        #for a in self.tagged_tokens_list: print(a)\n",
    "        self.n_vocab=len(self.token_counter.keys())\n",
    "        self.calc_prob()\n",
    "            \n",
    "    def assign_tag(self,token,params={}):\n",
    "        if token in [\"<s>\",\"</s>\"]: tag=token\n",
    "        elif token.isdigit(): tag=\"NUM\"\n",
    "        else: tag=random.choice(self.available_tags)\n",
    "        return tag\n",
    "    def calc_prob(self): #calculate P(word|tag) , P(tag|word), P(tag2|tag1)\n",
    "        tag_pairs=[(self.tagged_tokens_list[i0][1],self.tagged_tokens_list[i0+1][1]) for i0 in range(len(self.tagged_tokens_list)-1)] #get the tag pairs from the current assignment\n",
    "        sorted_by_word=sorted(self.tagged_tokens_list,key=lambda x:x[0])\n",
    "        sorted_by_tag=sorted(self.tagged_tokens_list,key=lambda x:x[1])\n",
    "        tag_pairs_sorted=sorted(tag_pairs,key=lambda x:x[0])\n",
    "        \n",
    "        grouped_by_word=dict(iter( [(key,dict( Counter([v[1] for v in list(group)]) )  ) for key,group in groupby(sorted_by_word,lambda x:x[0])] )) \n",
    "        grouped_by_tag=dict(iter( [(key,dict( Counter([v[0] for v in list(group)]) )  ) for key,group in groupby(sorted_by_tag,lambda x:x[1])] )) \n",
    "        tag_pairs_grouped_by_tag=dict(iter( [(key,dict( Counter([v[1] for v in list(group)]) )  ) for key,group in groupby(tag_pairs_sorted,lambda x:x[0])] )) \n",
    "        \n",
    "        # for a in grouped_by_word.items(): print(a)\n",
    "        # for a in grouped_by_tag.items(): print(a)\n",
    "        for a in tag_pairs: print(a)\n",
    "        \n",
    "        \n",
    "\n",
    "# Corpus\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"a dog barked loudly\",\n",
    "    \"many birds fly in the sky\",\n",
    "    \"children play in the garden\",\n",
    "    \"some dogs run fast\",\n",
    "    \"the girl jumped high\",\n",
    "    \"birds sing in trees\",\n",
    "    \"the children run fast\",\n",
    "    \"many children also play\",\n",
    "    \"the dog play with the children\",\n",
    "    \"the birds are on the trees\",\n",
    "    \"she jumped high\",\n",
    "    \"they play in the house\",\n",
    "    \"it sat on the table\",\n",
    "    \"there were 15 boys\",\n",
    "    \"there are also 12 men, 7 women\"\n",
    "]\n",
    "\n",
    "#tags=[\"N\",\"V\",\"PREP\",\"DET\",\"ADJ\",\"ADV\", \"<s>\",\"</s>\"]\n",
    "#available_tags=[\"N\",\"V\",\"PREP\",\"DET\",\"ADJ\",\"ADV\"]\n",
    "available_tags=[\"N\",\"V\",\"PREP\",\"DET\",\"OTHER\",\"NUM\",\"PUNC\"]\n",
    "all_tags=available_tags+[\"<s>\",\"</s>\"]\n",
    "\n",
    "\n",
    "shadow(sentences,available_tags)\n",
    "\n",
    "# all_tokenized=[]\n",
    "# all_token_pairs=[]\n",
    "# word_counter={}\n",
    "# for sent_i,sent0 in enumerate(sentences):\n",
    "#     tokens=tok_basic(sent0,add_sent_tags=True)\n",
    "#     all_tokenized.append(tokens)\n",
    "#     for tk0 in tokens: word_counter[tk0]=word_counter.get(tk0,0)+1\n",
    "#     for tk_i0,cur_tk0 in enumerate(tokens[:-1]):\n",
    "#         next_tk0=tokens[tk_i0+1]\n",
    "#         all_token_pairs.append((cur_tk0,next_tk0))\n",
    "\n",
    "# vocab=sorted(list(word_counter.keys())) \n",
    "\n",
    "# print(vocab)\n",
    "# word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "# idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "\n",
    "# #for a in all_token_pairs: print(a)\n",
    "\n",
    "# word_tag_list=[]\n",
    "# #tag_counter={}\n",
    "# # Tokenize corpus\n",
    "# for sent_i,sent_tokens in enumerate(all_tokenized):\n",
    "#     #print(sent_tokens)\n",
    "#     for tk0 in sent_tokens:\n",
    "#         if tk0 in [\"<s>\",\"</s>\"]: cur_tag=tk0 \n",
    "#         else: cur_tag = random.choice(available_tags)\n",
    "#         word_tag_list.append((sent_i,tk0,cur_tag))\n",
    "#         #tag_counter[cur_tag]=tag_counter.get(cur_tag,0)+1\n",
    "\n",
    "# sorted_by_word=sorted(word_tag_list,key=lambda x:x[1])\n",
    "# sorted_by_tag=sorted(word_tag_list,key=lambda x:x[-1])\n",
    "# #word_tag_list.sort(key=lambda x:x[1])\n",
    "# tag_pairs_list=[]\n",
    "# for i0,cur_item0 in enumerate(word_tag_list[:-1]):\n",
    "#     next_item0=word_tag_list[i0+1]\n",
    "#     if next_item0[0]!=cur_item0[0]: continue\n",
    "#     cur_tag_pair=cur_item0[-1],next_item0[-1]\n",
    "#     tag_pairs_list.append(cur_tag_pair)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1804ee5a-f5ed-436a-aa92-d3f6c16856d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fc2579c-fa2b-44be-84ed-2aa242ba443f",
   "metadata": {},
   "source": [
    "dir(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d57c2a-ba68-43c1-a702-d6e2167ad75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
